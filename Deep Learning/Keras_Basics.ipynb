{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras Basics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNe+Vmrvijz96x4yFfPM0+R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gumberankush/AI-DataScience-Course/blob/main/Deep%20Learning/Keras_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDO-YqZXjfGi"
      },
      "source": [
        "Reference: https://www.tensorflow.org/guide/keras/functional\n",
        "# Keras\n",
        "\n",
        "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "\n",
        "Use Keras if you need a deep learning library that:\n",
        "\n",
        "\n",
        "\n",
        "*   Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "*   Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "*   Runs seamlessly on CPU and GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSGrqgU3jnx_"
      },
      "source": [
        "## Keras backends\n",
        "\n",
        "What is a \"backend\"?\n",
        "\n",
        "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n",
        "\n",
        "At this time, Keras has three backend implementations available: **the TensorFlow backend, the Theano backend, and the CNTK backend**.\n",
        "\n",
        "TensorFlow is an open-source symbolic tensor manipulation framework developed by Google.\n",
        "\n",
        "Theano is an open-source symbolic tensor manipulation framework developed by LISA Lab at Université de Montréal. ( No further development on this framework)\n",
        "\n",
        "CNTK is an open-source toolkit for deep learning developed by Microsoft."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOaO4Pm3jwVe"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X45MKWoTmXnR"
      },
      "source": [
        "## Layers\n",
        "\n",
        "A Layer encapsulates a state (weights) and some computation (defined in the call method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qAayVSumfVZ"
      },
      "source": [
        "tf.keras.layers.Layer(\n",
        "    trainable=True, name=None, dtype=None, dynamic=False, **kwargs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfEnApwAm-9y"
      },
      "source": [
        "A layer is a class implementing common neural networks operations, such as convolution, batch norm, etc. These operations require managing weights, losses, updates, and inter-layer connectivity.\n",
        "\n",
        "Users will just instantiate a layer and then treat it as a callable.\n",
        "\n",
        "It is recommended that descendants of Layer implement the following methods:\n",
        "\n",
        "**__init__()**: Save configuration in member variables\n",
        "\n",
        "**build()**: Called once from __call__, when we know the shapes of inputs and dtype. Should have the calls to add_weight(), and then call the super's build() (which sets self.built = True, which is nice in case the user wants to call build() manually before the first __call__).\n",
        "\n",
        "**call()**: Called in __call__ after making sure build() has been called once. Should actually perform the logic of applying the layer to the input tensors (which should be passed in as the first argument)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIX_HJDvmvU_"
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Linear(Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32, input_dim=32): # constructor\n",
        "      super(Linear, self).__init__()\n",
        "      w_init = tf.random_normal_initializer() # weight's random normal intialization\n",
        "      self.w = tf.Variable(\n",
        "          initial_value=w_init(shape=(input_dim, units), dtype='float32'),\n",
        "          trainable=True)\n",
        "      b_init = tf.zeros_initializer() # bias intialized with Zero\n",
        "      self.b = tf.Variable(\n",
        "          initial_value=b_init(shape=(units,), dtype='float32'),\n",
        "          trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eke9e8ghnsCd"
      },
      "source": [
        "### Model \n",
        "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the **Sequential model**, a linear stack of layers. For more complex architectures, one should use the Keras functional API, which allows to build arbitrary graphs of layers.\n",
        "\n",
        "Example of Sequential Model is as folows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpPcjHNDntLs"
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS3AOOiroj2S"
      },
      "source": [
        "## Built-in layers\n",
        "\n",
        "Keras provides us with a [wide range of built-in layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/), so that you don't have to implement your own layers all the time.\n",
        "- Dense layers\n",
        "- Convolution layers\n",
        "- Transposed convolutions\n",
        "- Separateable convolutions\n",
        "- Average and max pooling\n",
        "- Global average and max pooling\n",
        "- LSTM, GRU (with built-in cuDNN acceleration)\n",
        "- BatchNormalization\n",
        "- Dropout\n",
        "- Attention\n",
        "- ConvLSTM2D\n",
        "\n",
        "\n",
        "Stacking is done through add()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_q66s9jovN1"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "# Optionally, the first layer can receive an `input_shape` argument:\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(500,)))\n",
        "# Afterwards, we do automatic shape inference:\n",
        "model.add(Dense(32))\n",
        "\n",
        "# This is identical to the following:\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=500))\n",
        "\n",
        "# And to the following:\n",
        "model = Sequential()\n",
        "model.add(Dense(32, batch_input_shape=(None, 500)))\n",
        "\n",
        "# Note that you can also omit the `input_shape` argument:\n",
        "# In that case the model gets built the first time you call `fit` (or other\n",
        "# training and evaluation methods).\n",
        "model = Sequential()\n",
        "model.add(Dense(32))  # layer 1\n",
        "model.add(Dense(32))  # layer 2\n",
        "model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError()) # can provide any kind of optimizer and loss\n",
        "# This builds the model for the first time:\n",
        "model.fit(x, y, batch_size=32, epochs=10) #training\n",
        "\n",
        "# Note that when using this delayed-build pattern (no input shape specified),\n",
        "# the model doesn't have any weights until the first call\n",
        "# to a training/evaluation method (since it isn't yet built):\n",
        "model = Sequential()\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(32))\n",
        "model.weights  # returns []\n",
        "\n",
        "# Whereas if you specify the input shape, the model gets built continuously\n",
        "# as you are adding layers:\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_shape=(500,)))\n",
        "model.add(Dense(32))\n",
        "model.weights  # returns list of length 4\n",
        "\n",
        "# When using the delayed-build pattern (no input shape specified), you can\n",
        "# choose to manually build your model by calling `build(batch_input_shape)`:\n",
        "model = Sequential()\n",
        "model.add(Dense(32))\n",
        "model.add(Dense(32))\n",
        "model.build((None, 500))\n",
        "model.weights  # returns list of length 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us834AL71Mev"
      },
      "source": [
        "Once the model is built let's configure the model's learning process with compile()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_ez5eBr1SyD"
      },
      "source": [
        "compile(\n",
        "    optimizer='rmsprop', loss=None, metrics=None, loss_weights=None,\n",
        "    sample_weight_mode=None, weighted_metrics=None, target_tensors=None,\n",
        "    distribute=None, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFEubX4h1inl"
      },
      "source": [
        "**optimizer**: String (name of optimizer) or optimizer instance. See tf.keras.optimizers.\n",
        "\n",
        "**loss**: String (name of objective function), objective function or tf.keras.losses.Loss instance. See tf.keras.losses. An objective function is any callable with the signature scalar_loss = fn(y_true, y_pred). If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses.\n",
        "\n",
        "**metrics**: List of metrics to be evaluated by the model during training and testing. Typically you will use metrics=['accuracy']. To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}. You can also pass a list (len = len(outputs)) of lists of metrics such as metrics=[['accuracy'], ['accuracy', 'mse']] or metrics=['accuracy', ['accuracy', 'mse']]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAuL_74p2PxU"
      },
      "source": [
        "**evaluate()**\n",
        "\n",
        "Returns the loss value & metrics values for the model in test mode.\n",
        "\n",
        "Computation is done in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf90rpSG2Sm2"
      },
      "source": [
        "evaluate(\n",
        "    x=None, y=None, batch_size=None, verbose=1, sample_weight=None, steps=None,\n",
        "    callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq-eR8ym2ZhX"
      },
      "source": [
        "**fit()**\n",
        "\n",
        "Trains the model for a fixed number of epochs (iterations on a dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAeT2hns2eJq"
      },
      "source": [
        "fit(\n",
        "    x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
        "    validation_split=0.0, validation_data=None, shuffle=True, class_weight=None,\n",
        "    sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
        "    validation_steps=None, validation_freq=1, max_queue_size=10, workers=1,\n",
        "    use_multiprocessing=False, **kwargs\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NNo2gF52fwN"
      },
      "source": [
        "We can now iterate on your training data in batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJl6gqFR2nKS"
      },
      "source": [
        "# dummy code\n",
        "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRn0qNzq2sjv"
      },
      "source": [
        "Evaluate your performance in one line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXkbIeYy2wbx"
      },
      "source": [
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCbqreQR2xQl"
      },
      "source": [
        "Or generate predictions on new data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iopUPQxr21UH"
      },
      "source": [
        "classes = model.predict(x_test, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}